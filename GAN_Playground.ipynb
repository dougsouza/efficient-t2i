{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_loaders.CUB_200_2011 import CUB200Dataset\n",
    "from models import get_gmodel\n",
    "from ganlib.priors import get_sampler_fn\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = Path(\"logs/EfficientGAN_96[ICJNN.exp1][AttnG.Interp.ttur]_CUB_0114_1725\")\n",
    "epoch = 440\n",
    "with open(model_folder / 'config.yaml', 'r') as f:\n",
    "        params = yaml.load(f, Loader=yaml.Loader)\n",
    "        config = edict(params)\n",
    "device = torch.device('cpu') # 'cuda' for GPU\n",
    "m = torch.load(\n",
    "    model_folder / f\"netG_avg_epoch_{epoch}.pth\",\n",
    "    map_location=lambda storage, loc: storage,\n",
    ")\n",
    "netG = get_gmodel(**m['netG_params'])\n",
    "netG.load_state_dict(m['net'])\n",
    "netG = netG.eval().to(device)\n",
    "sampler_fn = get_sampler_fn(\n",
    "    config.prior,\n",
    "    device=device,\n",
    "    normalize=config.normalize_prior,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "-------------\n",
    "We need to get text embeddings and to get real images if we want to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                             (0.5, 0.5, 0.5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dset = CUB200Dataset(\n",
    "    'datasets/CUB',\n",
    "    tform,\n",
    "    None,\n",
    "    split='test',\n",
    "    return_captions=True,\n",
    "    return_fnames=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing a real image from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_idx = 0 # from 0 to 9\n",
    "\n",
    "img, embs, caption, fname, fix = dset[0] # test instance idx\n",
    "embs = embs.squeeze(0)[cap_idx]\n",
    "emb = torch.tensor(embs[None, ...], dtype=torch.float32)\n",
    "cap = caption[cap_idx]\n",
    "print('fname:', fname, 'fix:', fix, 'cap idx:', cap_idx, 'caption:', cap)\n",
    "img = img.mul(0.5).add(0.5).clamp(0, 1).mul(255).clamp_(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating an image from the description above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = sampler_fn(1, config.z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = netG(z.to(device), emb.to(device))\n",
    "    img = imgs[0].mul(0.5).add(0.5).clamp(0, 1).mul(255).clamp_(0, 255).byte().permute(1, 2, 0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "plt.imshow(img)\n",
    "plt.text(0, -10, cap, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating multiple images for a single caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8 # number of images to generate\n",
    "z = sampler_fn(batch_size, config.z_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = netG(z.to(device), emb.to(device).repeat(batch_size, 1))\n",
    "    imgs = imgs.mul(0.5).add(0.5).clamp(0, 1).mul(255).clamp_(0, 255).byte().permute(0, 2, 3, 1).cpu().numpy()\n",
    "print(imgs.shape)\n",
    "\n",
    "mydpi=170.\n",
    "\n",
    "print(cap)\n",
    "fig, axes = plt.subplots(2, 4, sharex=True, sharey=True, figsize=((256*4./mydpi), (256*2./mydpi)), dpi=mydpi)\n",
    "axes.ravel()[0].text(0, -10, cap, fontsize=12)\n",
    "for ii, ax in zip(range(batch_size), axes.flatten()):\n",
    "    ax.imshow(imgs[ii], aspect='equal')\n",
    "    ax.set_axis_off()\n",
    "plt.subplots_adjust(bottom=0, top=1, left=0, right=1, wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
